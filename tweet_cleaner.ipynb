{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TurkishStemmer import TurkishStemmer\n",
    "from simplemma import text_lemmatizer\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = None\n",
    "with open(\"stopwords.txt\", \"r\") as stop_file:\n",
    "    stop_words = set(stop_file.read().splitlines())\n",
    "\n",
    "skip_words = None\n",
    "with open(\"skips.txt\", \"r\") as skip_file:\n",
    "    skip_words = set(skip_file.read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    text = text.replace(\"Kur’an\", \"kuran\")\n",
    "    text = text.replace(\"â\", \"a\")\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "    text = text.replace(\"İ\", \"i\")\n",
    "    text = text.replace(\"I\", \"ı\")\n",
    "    text = text.replace(u\"\\u00A0\", \" \")\n",
    "    text = text.replace(\"|\", \" \")\n",
    "\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \" \", text)\n",
    "    text = re.sub(r\"(.)\\1+\", r\"\\1\\1\", text)\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \" \", text)\n",
    "    text = re.sub(r\"http?:\\/\\/\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"#(\\w+)\", \" \", text)\n",
    "    text = re.sub(r\"^\\x00-\\x7F]+\", \" \", text)\n",
    "    text = re.sub(r\"[^A-Za-zâîığüşöçİĞÜŞÖÇ]+\", \" \", text)\n",
    "    text = re.sub(r\"((https://[^\\s]+))\", \" \", text)\n",
    "\n",
    "    text = \" \".join(text.lower().strip().split())\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = {}\n",
    "\n",
    "with open(f\"data/tweets_merged.json\", \"r\") as in_file:\n",
    "    all_tweets = json.load(in_file)\n",
    "\n",
    "for aday in all_tweets:\n",
    "    clean_tweets[aday] = []\n",
    "    tweets = all_tweets[aday]\n",
    "    for tweet in tweets:\n",
    "        cleaned_tweet = clean_tweet(tweet[\"rawContent\"])\n",
    "        if len(cleaned_tweet) > 0:\n",
    "            clean_tweets[aday].append(cleaned_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/clean_tweets.json\", \"w\") as out_file:\n",
    "    json.dump(clean_tweets, out_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = None\n",
    "\n",
    "with open(f\"data/clean_tweets.json\", \"r\") as in_file:\n",
    "    all_tweets = json.load(in_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lemms = {}\n",
    "\n",
    "for aday in all_tweets:\n",
    "    tweet_lemms[aday] = []\n",
    "    tweets = all_tweets[aday]\n",
    "    for tweet in tweets:\n",
    "        doc = text_lemmatizer(tweet, lang=\"tr\")\n",
    "        lemm_tweet = []\n",
    "        for lemma in doc:\n",
    "            if lemma not in stop_words:\n",
    "                lemm_tweet.append(lemma)\n",
    "                \n",
    "        if len(lemm_tweet) > 0:\n",
    "            tweet_lemms[aday].append(\" \".join(lemm_tweet))\n",
    "\n",
    "with open(f\"data/lemm_tweets.json\", \"w\") as out_file:\n",
    "    json.dump(tweet_lemms, out_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stems = {}\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "for aday in all_tweets:\n",
    "    tweet_stems[aday] = []\n",
    "    tweets = all_tweets[aday]\n",
    "    for tweet in tweets:\n",
    "        words = tweet.split()\n",
    "        stem_tweet = []\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "\n",
    "            if len(word) <= 6 or word in skip_words:\n",
    "                stem_tweet.append(word)\n",
    "            else:\n",
    "                stem_tweet.append(stemmer.stem(word))\n",
    "\n",
    "        if len(stem_tweet) > 0:\n",
    "            tweet_stems[aday].append(\" \".join(stem_tweet))\n",
    "\n",
    "\n",
    "with open(f\"data/stem_tweets.json\", \"w\") as out_file:\n",
    "    json.dump(tweet_stems, out_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
